{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48346b-042d-4e27-9e2c-c14a2bdedbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9dce8-4361-49eb-9edc-e9a89d526ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd368e15-27ff-4664-acfe-0e44176019bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4b0c6-3aba-4a43-b901-fb42dd723103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2aa1af9-2e46-4bc9-b959-a4c5ecff5444",
   "metadata": {},
   "source": [
    "Training on Lateral sided images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1b04d-4996-4f34-ba3a-8772e298233b",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b107c0fa-b570-4faf-83c3-34656f1e712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 11:30:03.465906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-04 11:30:03.465951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-04 11:30:03.467882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-04 11:30:03.478041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 11:30:04.290438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from tensorflow import concat\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM,Layer,Dropout,GRU\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import repeat\n",
    "from sklearn.utils import shuffle\n",
    "import nltk.translate.bleu_score as bleu\n",
    "import warnings\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print(\"done importing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ac00f-0bab-4a8f-a61b-fe1c1161fc45",
   "metadata": {},
   "source": [
    "File which consist image features extracted from chexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35b36c1-226e-46c5-8079-a29a28347e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eaae212-e017-451b-86f5-0617a5de1fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca520ad6-d960-4bde-ad22-ac9da0bd23b9",
   "metadata": {},
   "source": [
    "0 ===> filename <br>\n",
    "1 ===> uid <br>\n",
    "2 ===> findings<br>\n",
    "3 ===> image_features<br>\n",
    "4 ===> findings_total<br>\n",
    "5 ===> dec_ip<br>\n",
    "6 ===> dec_op<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7fe4c9-447e-45d3-99e3-03496b8dce9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6232"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c7e37f-dd91-4ed1-8e12-7ad83bf75395",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data.values , test_size = 0.2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71b174e-aa63-4a77-85e2-6b52ad2c72f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4985, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12001bf-152f-4caa-9637-d43df5e5ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1247, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd72d210-d736-4b0c-a203-4610962286a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t1  = Tokenizer( filters='!\"#$%&()*+,-/:;=?@[\\\\]^_`{|}~\\t\\n',oov_token='OOV')\n",
    "t1.fit_on_texts(X_train[:,4])\n",
    "vocab_size_imp = len(t1.word_index) + 1\n",
    "\n",
    "dec_inp = t1.texts_to_sequences(X_train[:,5])\n",
    "\n",
    "dec_inp = pad_sequences(dec_inp, maxlen=76, padding='post')\n",
    "\n",
    "dec_inp_cv = t1.texts_to_sequences(X_test[:,5])\n",
    "\n",
    "dec_inp_cv = pad_sequences(dec_inp_cv, maxlen=76, padding='post')\n",
    "\n",
    "dec_op = t1.texts_to_sequences(X_train[:,6])\n",
    "\n",
    "dec_op = pad_sequences(dec_op, maxlen=76, padding='post')\n",
    "\n",
    "dec_op_cv = t1.texts_to_sequences(X_test[:,6])\n",
    "\n",
    "dec_op_cv = pad_sequences(dec_op_cv, maxlen=76, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269a920-8143-4f9d-94a1-ff39fcb91994",
   "metadata": {},
   "source": [
    "Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a9b1ef-8b83-4bf8-9fde-b76b11b067c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2065"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dcedaae-4731-4a7d-aaf7-9a143a1e7f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.preprocessing.text.Tokenizer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "995bd02a-9a73-4dd5-90a8-70b15035194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t2.pickle', 'wb') as handle:\n",
    "    pickle.dump(t1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d11495f1-f8b7-4eef-bddf-197d727dd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = pd.read_pickle('/home/professor/Downloads/fromgit/Deadline/t2.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccbbd010-412a-43cd-bfb3-4fbf37fe6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "imp1 = {}\n",
    "imp2 = {}\n",
    "for key,value in t2.word_index.items():\n",
    "  imp1[value] = key\n",
    "  imp2[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "032b7740-5b52-46ee-b1e2-5dbf28dc6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('imp1.json', 'w') as file1:\n",
    "    json.dump(imp1, file1)\n",
    "\n",
    "# Save imp2 to a JSON file\n",
    "with open('imp2.json', 'w') as file2:\n",
    "    json.dump(imp2, file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24a318-2f98-4f1e-b413-e53ea650acc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe96b059-d2e1-434b-94e4-f41e605db2bd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##Model Design Of LLM##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91fe2ed1-54a5-4409-bea4-8f237e06e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.dense = Dense(self.units,name = 'Enc_dense')\n",
    "\n",
    "\n",
    "    def call(self,img):\n",
    "       \n",
    "      '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "      '''\n",
    "      print(\"ENcoder =======================================\")\n",
    "      #enc_out = self.maxpool(tf.expand_dims(img,axis = 2))\n",
    "      enc_out = self.dense(img)\n",
    "      print(\"IMAGE SHAPE IN ENCODER :- \",img.shape)\n",
    "      print(\"ENcoder =======================================END\")\n",
    "\n",
    "      return enc_out\n",
    "\n",
    "\n",
    "    def initialize_states(self,batch_size):\n",
    "    \n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      self.batch_size  = batch_size\n",
    "\n",
    "      self.enc_h =tf.zeros((self.batch_size, self.units))\n",
    "\n",
    "        \n",
    "      #self.enc_c = tf.zeros((self.batch_size, self.lstm_size))\n",
    "      return self.enc_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d543154-e698-469f-9cba-55f3c7a7e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,att_units):\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "    super().__init__()\n",
    "\n",
    "    self.att_units = att_units\n",
    "\n",
    "    self.w1 =  tf.keras.layers.Dense( self.att_units , name = 'w1')\n",
    "    self.w2 =  tf.keras.layers.Dense( self.att_units,name = 'w2')\n",
    "    self.v =  tf.keras.layers.Dense(1,name = 'v')\n",
    "      \n",
    "\n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    print(\"Attention ===============================\")\n",
    "    self.decoder_hidden_state = decoder_hidden_state\n",
    "    '''\n",
    "       decoder_hidden_state.shape =  (32, 256)\n",
    "    '''\n",
    "    self.encoder_output = encoder_output\n",
    "    '''\n",
    "        encoder_output.shape =  (32, 256)\n",
    "    '''\n",
    "    print(\"Atten layer\")\n",
    "    print(\"Dec shape \",decoder_hidden_state.shape, \"Enc shape\", encoder_output.shape )\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "    self.decoder_hidden_state = tf.expand_dims(self.decoder_hidden_state,axis = 1)\n",
    "    score = self.v(tf.nn.tanh(\n",
    "              self.w1(self.decoder_hidden_state) + self.w2(self.encoder_output)))\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    '''\n",
    "        attention_weights.shape = (32, 32, 1)\n",
    "    '''\n",
    "    context_vector = attention_weights * self.encoder_output\n",
    "    \n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    '''\n",
    "        context_vector.shape = (32, 256)\n",
    "    '''\n",
    "    print(\"attention_weights.shape = \", attention_weights.shape)\n",
    "    print(\"context_vector.shape = \", context_vector.shape)\n",
    "    print(\"encoder_output.shape = \", encoder_output.shape)\n",
    "    print(\"decoder_hidden_state.shape = \", decoder_hidden_state.shape)\n",
    "    \n",
    "    print(\"Attention ===============================END\")\n",
    "    return context_vector,attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b22d54d6-f455-44aa-bbdd-41df5ff53dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "\n",
    "class OneStepDecoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units  ,att_units):\n",
    "\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "      super().__init__()\n",
    "      self.tar_vocab_size = tar_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units = dec_units\n",
    "      self.att_units = att_units\n",
    "      self.dec_emb = Embedding(tar_vocab_size,embedding_dim,trainable = True , name = 'dec_embb')\n",
    "      self.dec_lstm = GRU(self.dec_units, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "      self.dense   = Dense(self.tar_vocab_size, name = 'one_dec')\n",
    "      self.attention=Attention( self.att_units)\n",
    "      self.d1 = Dropout(0.3,name = 'd1')\n",
    "      self.d2 = Dropout(0.3,name = 'd2')\n",
    "      self.d3 = Dropout(0.3,name = 'd3')\n",
    "\n",
    "            \n",
    "\n",
    "  @tf.function\n",
    "  def call(self,input_to_decoder, encoder_output, state_h):\n",
    "    print(\"ONestepDecoder ================\")\n",
    "    '''\n",
    "         One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "    '''\n",
    "    self.input_to_decoder = input_to_decoder\n",
    "    '''\n",
    "        input_to_decoder.shape = (32, None)\n",
    "    '''\n",
    "    self.encoder_output = encoder_output\n",
    "    '''\n",
    "        encoder_output.shape = (32, 256)\n",
    "    '''\n",
    "    self.state_h = state_h\n",
    "    '''\n",
    "        state_h.shape = (32,256)\n",
    "    '''\n",
    "    #A\n",
    "    target_embedd           = self.dec_emb (self.input_to_decoder)     #(batch_size,1,embedingdim)\n",
    "    #B\n",
    "    target_embedd = self.d1(target_embedd)\n",
    "    '''\n",
    "        target_embedd.shape = (32, None, 300)\n",
    "    '''\n",
    "\n",
    "    context_vector,attention_weights=self.attention(self.state_h,self.encoder_output) #context vector shape = (batch_size,units)\n",
    "    '''\n",
    "        context_vector.shape = (32, 256)\n",
    "        attention_weights.shape = (32, 32, 1)\n",
    "        \n",
    "    '''\n",
    "    #C\n",
    "    concated = tf.concat([  tf.expand_dims(context_vector, 1),target_embedd], -1)\n",
    "    concated = self.d2(concated)\n",
    "    '''\n",
    "        concated.shape = (32, 1, 556)\n",
    "    '''\n",
    "\n",
    "    #D\n",
    "    lstm_output, hs      = self.dec_lstm(concated, initial_state=self.state_h)\n",
    "    '''\n",
    "        lstm_output.shape = (32, 1, 256)\n",
    "        hs.shape = (32, 256)\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "    lstm_output = tf.reshape(lstm_output, (-1, lstm_output.shape[2]))\n",
    "    lstm_output = self.d3(lstm_output)\n",
    "\n",
    "    '''\n",
    "        lstm_output.shape = (32, 256)\n",
    "    '''\n",
    "    #E\n",
    "    op = self.dense(lstm_output)\n",
    "    '''\n",
    "        op.shape = (32, 2053)\n",
    "    '''\n",
    "    #op = tf.squeeze(op,[1])\n",
    "    \n",
    "    print(\"input_to_decoder :- \", input_to_decoder.shape , \"state_h :- \", state_h.shape)\n",
    "    \n",
    "    print(\"encoder_output \", state_h.shape, \"Target eMBEDING - 2\",target_embedd.shape )\n",
    "    print(\"Concat :- \", concated.shape)\n",
    "    print(\"Lstm output\",lstm_output.shape, \"OP\", op.shape )\n",
    "    print(\"HS:- \", hs.shape)\n",
    "    print(\"context_vector.shape\", context_vector.shape,\"attention_weights.shape\", attention_weights.shape )\n",
    "    \n",
    "    print(\"ONestepDecoder ================END\")\n",
    "\n",
    "    # print(\"dec_lstm:- \", dec_lstm)\n",
    "    return op,hs,attention_weights,context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbec3054-8200-4349-98d0-74b447dfded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "        out_vocab_size = 2053\n",
    "        embedding_dim = 300\n",
    "        input_length = 76\n",
    "        dec_units = 256\n",
    "        att_units = 64\n",
    "    '''\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      super().__init__()\n",
    "      self.out_vocab_size = out_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units = dec_units\n",
    "      self.att_units = att_units\n",
    "      self.onestep = OneStepDecoder(self.out_vocab_size,self.embedding_dim ,self.input_length,self.dec_units,self.att_units)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state):\n",
    "\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "\n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        print(\"DECODER =======================\")\n",
    "        all_outputs = tf.TensorArray(tf.float32,size =input_to_decoder.shape[1],name = 'output_arrays' )\n",
    "        '''\n",
    "            all_outputs.shape = (32, 76, 2053)\n",
    "        '''\n",
    "        self.input_to_decoder = input_to_decoder\n",
    "        '''\n",
    "            input_to_decoder.shape = (32, 76)\n",
    "        '''\n",
    "        self.encoder_output = encoder_output\n",
    "        '''\n",
    "            encoder_output = (32, 256)\n",
    "        '''\n",
    "        \n",
    "        self.decoder_hidden_state = decoder_hidden_state\n",
    "        '''\n",
    "            decoder_hidden_state.shape = (32, 256)\n",
    "        '''\n",
    "\n",
    "        for timestep in tf.range(input_to_decoder.shape[1]):\n",
    "          op,hs,attention_weights,context_vector = self.onestep(self.input_to_decoder[:,timestep:timestep+1], \n",
    "                                                                self.encoder_output, \n",
    "                                                                self.decoder_hidden_state\n",
    "                                                               )\n",
    "          self.decoder_hidden_state = hs\n",
    "          all_outputs = all_outputs.write(timestep,op)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        '''\n",
    "            all_outputs.shape = (32, 76, 2053)\n",
    "        '''\n",
    "        '''\n",
    "            context_vector.shape = (32,256)\n",
    "        '''\n",
    "        print(\"context_vector. shape = \", context_vector.shape)\n",
    "\n",
    "        print(\"input_to_decoder\" , input_to_decoder.shape)\n",
    "        print(\"decoder_hidden_state\", decoder_hidden_state.shape)\n",
    "        print(\"all_outputs\", all_outputs.shape)\n",
    "        print(\"encoder_output = \", encoder_output.shape)\n",
    "        print(\"DECODER =======================END\")\n",
    "        return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "969b9b9a-1fa3-4d53-8003-b2b0f89246d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class encoder_decoder(tf.keras.Model):\n",
    "  #def __init__(self,#params):\n",
    "    #Intialize objects from encoder decoder\n",
    "  \n",
    "  '''out_vocab_size = 2053\n",
    "      embedding_size_d = 300\n",
    "      input_length_d = 76\n",
    "      lstm_size_d = 256\n",
    "      att_units = 64\n",
    "      batch_size = 32\n",
    "      units = 256\n",
    "  '''  \n",
    "  def __init__(self,out_vocab_size , embedding_size_d, input_length_d,lstm_size_d,att_units,batch_size,units):\n",
    "\n",
    "        #Create encoder object\n",
    "        #Create decoder object\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.units = units\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.embedding_size_d = embedding_size_d\n",
    "        self.lstm_size_d = lstm_size_d\n",
    "        self.input_length_d = input_length_d\n",
    "        self.batch_size = batch_size\n",
    "        self.att_units = att_units\n",
    "\n",
    "        self.encoder = Encoder(self.units)\n",
    "\n",
    "        self.decoder = Decoder(out_vocab_size , embedding_size_d, input_length_d,lstm_size_d,att_units )\n",
    "        self.dense   = Dense(self.out_vocab_size,name = 'enc_dec_dense')\n",
    "\n",
    "\n",
    "\n",
    "  def call(self,data):\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output\n",
    "    print(\"encoder_decoder ===================\")\n",
    "    self.inputs,self.outputs = data[0], data[1]\n",
    "    '''\n",
    "        Data[0] value  =  (32,1024)\n",
    "        Data[1] value  =  (32,76)\n",
    "        \n",
    "    '''\n",
    "    # print(\"=\"*20, \"ENCODER\", \"=\"*20)\n",
    "    self.encoder_h= self.encoder.initialize_states(self.batch_size)\n",
    "    self.encoder_output = self.encoder(self.inputs)\n",
    "    '''\n",
    "        encoder_output.shape = (32,256)\n",
    "        encoder_h.shape = (32,256)\n",
    "    '''\n",
    "    print(\"-\"*27)\n",
    "    # print(\"ENCODER ==> OUTPUT SHAPE\",self.encoder_output.shape)\n",
    "    # print(\"ENCODER ==> HIDDEN STATE SHAPE\",self.encoder_h.shape)\n",
    "    # print(\"=\"*20, \"DECODER\", \"=\"*20)\n",
    "    output= self.decoder(self.outputs,self.encoder_output,self.encoder_h)\n",
    "    '''\n",
    "        output.shape = (32, 76, 2053)\n",
    "        \n",
    "    '''\n",
    "    # print(\"FINAL OUTPUT SHAPE\",output.shape)\n",
    "    print(\"data[0], data[1]  = \", data[0].shape, data[1].shape)\n",
    "    print(\"output.shape = \", output.shape)\n",
    "    print(\"encoder_decoder =================== END\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df6aae7f-a786-4b3a-aeaf-f5e60c7a6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "  #out_vocab_size , embedding_size_d, input_length_d,lstm_size_d,att_units,batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfd19684-28eb-4468-b999-6c40d3c3cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoint’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ba7bb6d-0278-482d-909d-159dbf6d465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint filepath: /home/professor/Downloads/fromgit/Deadline/checkpoint/model_checkpoint.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Define checkpoint directory and file path\n",
    "checkpoint_dir = os.path.join(cwd, 'checkpoint')\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, 'model_checkpoint.keras')\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Print checkpoint filepath to verify\n",
    "print(\"Checkpoint filepath:\", checkpoint_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4eb5459-edf1-4c68-b571-46cb349f7e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a87060b-b31b-48ad-b651-a961c403f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8,mode = 'min',verbose = 1,\n",
    "                              patience=2, min_lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8cb737c-c970-4bbc-8f42-c9b0451f50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = encoder_decoder(vocab_size_imp,300,76,256,64,32,256)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model_1.compile(optimizer=optimizer,loss=loss_function, metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4a207ef-51fc-4322-8db1-a748a8abfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp = np.vstack(X_train[:,3]).astype(float)  # Use np.float64 for double-precision floats\n",
    "test_inp = np.vstack(X_test[:,3]).astype(float)  # Use np.float32 for single-precision floats (if memory efficiency is a concern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ecaeb1c-141d-41dd-8228-2a0e86431a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inp shape: (4985, 1024)\n",
      "dec_inp shape: (4985, 76)\n",
      "dec_op shape: (4985, 76)\n",
      "test_inp shape: (1247, 1024)\n",
      "dec_inp_cv shape: (1247, 76)\n",
      "dec_op_cv shape: (1247, 76)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of your data\n",
    "print(\"train_inp shape:\", train_inp.shape)\n",
    "print(\"dec_inp shape:\", dec_inp.shape)\n",
    "print(\"dec_op shape:\", dec_op.shape)00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "print(\"test_inp shape:\", test_inp.shape)\n",
    "print(\"dec_inp_cv shape:\", dec_inp_cv.shape)\n",
    "print(\"dec_op_cv shape:\", dec_op_cv.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02f877-770a-44eb-9303-c81c3745c985",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "model_1.fit([train_inp,dec_inp ],dec_op ,validation_data= ([test_inp, dec_inp_cv],dec_op_cv),batch_size= 32,epochs  = 1,callbacks=[reduce_lr,model_checkpoint_callback] , shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54895620-c9f5-4b34-a32b-fd55611909aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_decoder ===================\n",
      "ENcoder =======================================\n",
      "IMAGE SHAPE IN ENCODER :-  (None, 1024)\n",
      "ENcoder =======================================END\n",
      "---------------------------\n",
      "DECODER =======================\n",
      "ONestepDecoder ================\n",
      "Attention ===============================\n",
      "Atten layer\n",
      "Dec shape  (32, 256) Enc shape (None, 256)\n",
      "attention_weights.shape =  (32, None, 1)\n",
      "context_vector.shape =  (32, 256)\n",
      "encoder_output.shape =  (None, 256)\n",
      "decoder_hidden_state.shape =  (32, 256)\n",
      "Attention ===============================END\n",
      "input_to_decoder :-  (None, None) state_h :-  (32, 256)\n",
      "encoder_output  (32, 256) Target eMBEDING - 2 (None, None, 300)\n",
      "Concat :-  (32, 1, 556)\n",
      "Lstm output (32, 256) OP (32, 2065)\n",
      "HS:-  (32, 256)\n",
      "context_vector.shape (32, 256) attention_weights.shape (32, None, 1)\n",
      "ONestepDecoder ================END\n",
      "ONestepDecoder ================\n",
      "Attention ===============================\n",
      "Atten layer\n",
      "Dec shape  (32, 256) Enc shape (None, 256)\n",
      "attention_weights.shape =  (32, None, 1)\n",
      "context_vector.shape =  (32, 256)\n",
      "encoder_output.shape =  (None, 256)\n",
      "decoder_hidden_state.shape =  (32, 256)\n",
      "Attention ===============================END\n",
      "input_to_decoder :-  (None, None) state_h :-  (32, 256)\n",
      "encoder_output  (32, 256) Target eMBEDING - 2 (None, None, 300)\n",
      "Concat :-  (32, 1, 556)\n",
      "Lstm output (32, 256) OP (32, 2065)\n",
      "HS:-  (32, 256)\n",
      "context_vector.shape (32, 256) attention_weights.shape (32, None, 1)\n",
      "ONestepDecoder ================END\n",
      "context_vector. shape =  (32, 256)\n",
      "input_to_decoder (None, 76)\n",
      "decoder_hidden_state (32, 256)\n",
      "all_outputs (32, 76, 2065)\n",
      "encoder_output =  (None, 256)\n",
      "DECODER =======================END\n",
      "data[0], data[1]  =  (None, 1024) (None, 76)\n",
      "output.shape =  (32, 76, 2065)\n",
      "encoder_decoder =================== END\n",
      "encoder_decoder ===================\n",
      "ENcoder =======================================\n",
      "IMAGE SHAPE IN ENCODER :-  (None, 1024)\n",
      "ENcoder =======================================END\n",
      "---------------------------\n",
      "data[0], data[1]  =  (None, 1024) (None, 76)\n",
      "output.shape =  (32, 76, 2065)\n",
      "encoder_decoder =================== END\n",
      " 41/156 [======>.......................] - ETA: 1:02 - loss: 2.2931 - accuracy: 0.0169"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdec_op\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp_cv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdec_op_cv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_1.fit([train_inp,dec_inp ],dec_op ,validation_data= ([test_inp, dec_inp_cv],dec_op_cv),batch_size= 32,epochs  = 1,callbacks=[reduce_lr,model_checkpoint_callback] , shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8df1be-0ca2-42aa-8992-ca3bfec0c58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
