{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3f022e-e48f-4a85-9780-e9be8df6050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 21:05:12.871325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-01 21:05:12.871410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-01 21:05:12.873032: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-01 21:05:12.883641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 21:05:13.982697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM,Layer,Dropout,GRU\n",
    "\n",
    "\n",
    "print(\"done importing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0366e367-686f-4ef6-a600-d6ad9e33433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.dense = Dense(self.units,name = 'Enc_dense')\n",
    "\n",
    "\n",
    "    def call(self,img):\n",
    "      '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "      '''\n",
    "      #enc_out = self.maxpool(tf.expand_dims(img,axis = 2))\n",
    "      enc_out = self.dense(img)\n",
    "      return enc_out\n",
    "\n",
    "\n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      self.batch_size  = batch_size\n",
    "\n",
    "      self.enc_h =tf.zeros((self.batch_size, self.units))\n",
    "\n",
    "      #self.enc_c = tf.zeros((self.batch_size, self.lstm_size))\n",
    "      return self.enc_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1292d0-0244-4c69-9aff-a545e78281f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,att_units):\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "    super().__init__()\n",
    "\n",
    "    self.att_units = att_units\n",
    "\n",
    "    self.w1 =  tf.keras.layers.Dense( self.att_units , name = 'w1')\n",
    "    self.w2 =  tf.keras.layers.Dense( self.att_units,name = 'w2')\n",
    "    self.v =  tf.keras.layers.Dense(1,name = 'v')\n",
    "\n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    self.decoder_hidden_state = decoder_hidden_state\n",
    "    self.encoder_output = encoder_output\n",
    "\n",
    "\n",
    "    self.decoder_hidden_state = tf.expand_dims(self.decoder_hidden_state,axis = 1)\n",
    "    score = self.v(tf.nn.tanh(\n",
    "              self.w1(self.decoder_hidden_state) + self.w2(self.encoder_output)))\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * self.encoder_output\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector,attention_weights\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "\n",
    "class OneStepDecoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units  ,att_units):\n",
    "\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "      super().__init__()\n",
    "      self.tar_vocab_size = tar_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units = dec_units\n",
    "      self.att_units = att_units\n",
    "      self.dec_emb = Embedding(tar_vocab_size,embedding_dim,trainable = True , name = 'dec_embb')\n",
    "      self.dec_lstm = GRU(self.dec_units, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "      self.dense   = Dense(self.tar_vocab_size, name = 'one_dec')\n",
    "      self.attention=Attention( self.att_units)\n",
    "      self.d1 = Dropout(0.3,name = 'd1')\n",
    "      self.d2 = Dropout(0.3,name = 'd2')\n",
    "      self.d3 = Dropout(0.3,name = 'd3')\n",
    "\n",
    "  @tf.function\n",
    "  def call(self,input_to_decoder, encoder_output, state_h):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "    '''\n",
    "    self.input_to_decoder = input_to_decoder\n",
    "    self.encoder_output = encoder_output\n",
    "    self.state_h = state_h\n",
    "\n",
    "    #A\n",
    "    target_embedd           = self.dec_emb (self.input_to_decoder)     #(batch_size,1,embedingdim)\n",
    "    #B\n",
    "    target_embedd = self.d1(target_embedd)\n",
    "\n",
    "    context_vector,attention_weights=self.attention(self.state_h,self.encoder_output) #context vector shape = (batch_size,att_units)\n",
    "    #C\n",
    "    concated = tf.concat([  tf.expand_dims(context_vector, 1),target_embedd], -1)\n",
    "    concated = self.d2(concated)\n",
    "\n",
    "    #D\n",
    "    lstm_output, hs      = self.dec_lstm(concated, initial_state=self.state_h)\n",
    "\n",
    "    lstm_output = tf.reshape(lstm_output, (-1, lstm_output.shape[2]))\n",
    "    lstm_output = self.d3(lstm_output)\n",
    "    #E\n",
    "    op = self.dense(lstm_output)\n",
    "    #op = tf.squeeze(op,[1])\n",
    "    return op,hs,attention_weights,context_vector\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      super().__init__()\n",
    "      self.out_vocab_size = out_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units = dec_units\n",
    "      self.att_units = att_units\n",
    "      self.onestep = OneStepDecoder(self.out_vocab_size,self.embedding_dim ,self.input_length,self.dec_units,self.att_units)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state):\n",
    "\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "\n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "\n",
    "        all_outputs = tf.TensorArray(tf.float32,size =input_to_decoder.shape[1],name = 'output_arrays' )\n",
    "        self.input_to_decoder = input_to_decoder\n",
    "        self.encoder_output = encoder_output\n",
    "        self.decoder_hidden_state = decoder_hidden_state\n",
    "\n",
    "        for timestep in tf.range(input_to_decoder.shape[1]):\n",
    "          op,hs,attention_weights,context_vector = self.onestep(self.input_to_decoder[:,timestep:timestep+1], self.encoder_output, self.decoder_hidden_state)\n",
    "          self.decoder_hidden_state = hs\n",
    "          all_outputs = all_outputs.write(timestep,op)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class encoder_decoder(tf.keras.Model):\n",
    "  #def __init__(self,#params):\n",
    "    #Intialize objects from encoder decoder\n",
    "  def __init__(self,out_vocab_size , embedding_size_d, input_length_d,lstm_size_d,att_units,batch_size,units):\n",
    "\n",
    "        #Create encoder object\n",
    "        #Create decoder object\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.units = units\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.embedding_size_d = embedding_size_d\n",
    "        self.lstm_size_d = lstm_size_d\n",
    "        self.input_length_d = input_length_d\n",
    "        self.batch_size = batch_size\n",
    "        self.att_units = att_units\n",
    "\n",
    "        self.encoder = Encoder(self.units)\n",
    "\n",
    "        self.decoder = Decoder(out_vocab_size , embedding_size_d, input_length_d,lstm_size_d,att_units )\n",
    "        #self.dense   = TimeDistributed(Dense(self.out_vocab_size, activation='softmax'))\n",
    "        self.dense   = Dense(self.out_vocab_size,name = 'enc_dec_dense')\n",
    "\n",
    "\n",
    "\n",
    "  def call(self,data):\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output\n",
    "    self.inputs,self.outputs = data[0], data[1]\n",
    "    print(\"=\"*20, \"ENCODER\", \"=\"*20)\n",
    "    self.encoder_h= self.encoder.initialize_states(self.batch_size)\n",
    "    self.encoder_output = self.encoder(self.inputs)\n",
    "    print(\"-\"*27)\n",
    "    print(\"ENCODER ==> OUTPUT SHAPE\",self.encoder_output.shape)\n",
    "    print(\"ENCODER ==> HIDDEN STATE SHAPE\",self.encoder_h.shape)\n",
    "    print(\"=\"*20, \"DECODER\", \"=\"*20)\n",
    "    output= self.decoder(self.outputs,self.encoder_output,self.encoder_h)\n",
    "    print(\"-\"*27)\n",
    "    print(\"FINAL OUTPUT SHAPE\",output.shape)\n",
    "    print(\"=\"*50)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d27fcf-be8f-4988-95b7-80de61fd41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "t2 = pd.read_pickle('/home/professor/Downloads/fromgit/Deadline/t2.pickle')\n",
    "\n",
    "imp1 = {}\n",
    "imp2 = {}\n",
    "for key,value in t2.word_index.items():\n",
    "  imp1[value] = key\n",
    "  imp2[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdfba95-2bd2-4d6c-9e78-54772e58fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 21:05:51.142204: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-07-01 21:05:51.142270: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: professor-Aspire-A715-42G\n",
      "2024-07-01 21:05:51.142288: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: professor-Aspire-A715-42G\n",
      "2024-07-01 21:05:51.142516: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.183.1\n",
      "2024-07-01 21:05:51.142545: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.183.1\n",
      "2024-07-01 21:05:51.142552: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.183.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chex_weights = '/home/professor/Downloads/fromgit/archive/chexweights.h5'\n",
    "chexnet = DenseNet121(weights='/home/professor/Downloads/fromgit/archive/chexweights.h5',                    \n",
    "                      classes = 14,input_shape=(224,224,3))\n",
    "model = Model(chexnet.input, chexnet.layers[-2].output)\n",
    "print(\"Model loded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754209cc-6863-47bb-bb55-a0b3e96779f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== ENCODER ====================\n",
      "---------------------------\n",
      "ENCODER ==> OUTPUT SHAPE (32, 256)\n",
      "ENCODER ==> HIDDEN STATE SHAPE (32, 256)\n",
      "==================== DECODER ====================\n",
      "---------------------------\n",
      "FINAL OUTPUT SHAPE (32, 76, 2053)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = '/home/professor/Downloads/fromgit/Deadline/checkpoint/model_checkpoint.keras'\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "  #out_vocab_size , embedding_size_d, input_length_d,lstm_size_d,att_units,batch_size)\n",
    "model_1_loaded = tf.keras.models.load_model(checkpoint_filepath, custom_objects={'loss_function': loss_function})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1913493f-86bd-4bb9-8765-d829e121a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    img = keras_image.load_img(image_path, target_size=(224, 224))  # InceptionV3 expects 299x299 images\n",
    "    img_array = keras_image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    features = model.predict(img_array)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0775635-096f-4f0b-b165-233c4ba50c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam(sentence):\n",
    "    \"\"\"This function predicts the sentence using beam search\"\"\"\n",
    "    initial_state = model_1_loaded.layers[0].initialize_states(1)\n",
    "    encoder_output = model_1_loaded.layers[0](sentence)\n",
    "    result = ''\n",
    "    sequences = [['<start>', initial_state, 0]]\n",
    "    decoder_hidden_state = initial_state\n",
    "    finished_seq = []\n",
    "    beam_width = 3\n",
    "    for i in range(79):\n",
    "        all_candidates = []\n",
    "        new_seq = []\n",
    "        for s in sequences:\n",
    "            cur_vec = np.reshape(imp2[s[0].split(\" \")[-1]], (1, 1))\n",
    "            decoder_hidden_state = s[1]\n",
    "            op, hs, attention_weights, context_vector = model_1_loaded.layers[1].onestep(cur_vec, encoder_output, decoder_hidden_state)\n",
    "            op = tf.nn.softmax(op)\n",
    "            top3 = np.argsort(op).flatten()[-beam_width:]\n",
    "            for t in top3:\n",
    "                candidates = [s[0] + ' ' + imp1[t], hs, s[2] - np.log(np.array(op).flatten()[t])]\n",
    "                all_candidates.append(candidates)\n",
    "        sequences = sorted(all_candidates, key=lambda l: l[2])[:beam_width]\n",
    "        count = 0\n",
    "        for s1 in sequences:\n",
    "            if s1[0].split(\" \")[-1] == '<end>':\n",
    "                s1[2] = s1[2] / len(s1[0])  # normalized\n",
    "                finished_seq.append([s1[0], s1[1], s1[2]])\n",
    "                count += 1\n",
    "            else:\n",
    "                new_seq.append([s1[0], s1[1], s1[2]])\n",
    "        beam_width -= count\n",
    "        sequences = new_seq\n",
    "        if not sequences:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    if len(finished_seq) > 0:\n",
    "        sequences = finished_seq[-1]\n",
    "        return sequences[0]\n",
    "    else:\n",
    "        return new_seq[-1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d324f1c-94ef-45f5-8947-df03d45b5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Sentence is:  <start> the no no <end>\n"
     ]
    }
   ],
   "source": [
    "def predict_caption_for_image(image_path):\n",
    "    img_features = load_and_preprocess_image(image_path)\n",
    "    prediction = beam(img_features)\n",
    "    print(\"Predicted Sentence is: \", prediction)\n",
    "\n",
    "# Test the function with an example image path\n",
    "example_image_path = \"/home/professor/Downloads/fromgit/archive/images/images_normalized/2_IM-0652-1001.dcm.png\"  # Replace with the actual image path\n",
    "predict_caption_for_image(example_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c3f84-6ee9-490d-b78c-7dc4201a1cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
